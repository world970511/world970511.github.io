{
  "project9": {
    "year": 2024,
    "company": "Posicube",
    "ko": {
      "title": "데이터 관리 플랫폼 구축",
      "motivation": "사내 여러 팀이 공통으로 사용하는 OCR 학습 데이터를 효율적으로 관리하기 위해 온프레미스 파일 서버의 파일 및 디렉토리를 데이터베이스화하여 검색, 관리, 동기화를 원활하게 수행하기 위한 웹 기반 파일 관리 시스템을 기획·설계·구현했습니다.<br>2024년 초부터 2025년 8월 퇴사 때까지 총 2인으로 진행되었고, 그 중 9개월은 1인 풀스택으로 진행하였습니다.<br>상단 블로그 링크에서 아키텍쳐 및 자세한 내용 확인이 가능합니다.",
      "features": [
        "파일/폴더 관리",
        "검색 및 조회",
        "이미지 처리",
        "자동화 (Celery Beat)",
        "관리자 기능",
        "부가 기능"
      ],
      "tech": "Python, Django, React, Celery, Redis, PostgreSQL, Meilisearch, Docker, Docker Compose, Nginx, JWT, Session, Redux",
      "results": [
        "사내 테스트 서버 배포",
        "팀 내 테스트 환경 구축 및 테스트 진행"
      ],
      "github": "",
      "blog": "https://world970511.github.io/blog/posts/2025-12-29-c1.html"
    },
    "en": {
      "title": "Data Management Platform Construction",
      "motivation": "I participated in the planning, design, and implementation of a web-based file management system to efficiently manage OCR training data used by multiple teams within the company. The project spanned from the beginning of 2024 to my departure from the company in August 2025, with a total of two developers working on it, and 9 months were dedicated to full-stack development by one developer.\nYou can check the architecture and details at the blog link above.",
      "features": [
        "File/folder management",
        "Search and retrieval",
        "Image processing",
        "Automation (Celery Beat)",
        "Administrator function",
        "Additional function"
      ],
      "tech": "Python, Django, React, Celery, Redis, PostgreSQL, Meilisearch, Docker, Docker Compose, Nginx, JWT, Session, Redux",
      "results": [
        "Deployment of internal test server",
        "Construction and testing of team internal test environment"
      ],
      "github": "",
      "blog": "https://world970511.github.io/blog/posts/2025-12-29-c1.html"
    }
  },
  "project8": {
    "year": 2025,
    "company": "",
    "ko": {
      "title": "DayFlow",
      "motivation": "구글 AI 스튜디오를 사용하여 만든 일정 관리 웹앱을 실제 안드로이드 앱으로 만들어 사용해보고자 하였습니다.<br>개인적으로 강제적으로 일정을 정리하고, 회고를 할 수 있는 앱의 필요성을 느끼고 있어 이 부분을 구현하는데 집중했고 평소 자주 사용하는 뽀모도로 타이머와 일반 타이머를 추가했습니다.<br>나아가 SUNO를 사용해 타이머 사용 시 작업하면서 집중하기 좋은 배경음을 만들어 추가했습니다.",
      "features": [
        "지정된 시간에 자동으로 오늘의 일정을 정리해달라고 요청하고, 이를 todo 리스트로 만들어 관리합니다",
        "지정된 시간에 오늘 하루가 어땠는지 회고를 요청하고, 어느 정도로 일을 끝냈는지 알려줍니다",
        "월간 기록을 제공하여 월간 회고를 할 수 있도록 합니다",
        "월간 기록을 다이어리 꾸미기를 할 수 있는 이미지로 다운받을 수 있습니다.",
        "뽀모도로 타이머와 일반 타이머를 원하는 배경음악을 들으며 사용하도록 작업했습니다."
      ],
      "tech": "Java, TypeScript, Android studio, Google AI Studio",
      "results": [
        "Google Play Store에 어플리케이션 업로드 준비 중"
      ],
      "github": "https://github.com/world970511/DayFlow",
      "blog": ""
    },
    "en": {
      "title": "DayFlow",
      "motivation": "I wanted to convert a schedule management web app created using Google AI Studio into a real Android application. Feeling a personal need for an app that facilitates structured scheduling and reflection, I focused on implementing these aspects and added frequently used features like a Pomodoro timer and a standard timer. Furthermore, I used SUNO to generate and include background music designed to improve concentration while using the timers.",
      "features": [
        "Automatically requests a summary of today&#39;s schedule at a specified time and manages it as a to-do list.",
        "Requests a reflection on the day at a specified time and provides feedback on task completion.",
        "Provides monthly records to enable monthly reflections.",
        "Allows users to download monthly records as images for digital journaling/decorating.",
        "Includes Pomodoro and standard timers with customizable background music for better focus."
      ],
      "tech": "Java, TypeScript, Android studio, Google AI Studio",
      "results": [
        "Application upload to Google Play Store is in progress"
      ],
      "github": "https://github.com/world970511/DayFlow",
      "blog": ""
    }
  },
  "project7": {
    "year": 2025,
    "company": "",
    "ko": {
      "title": "AI 웹소설 추천 시스템",
      "motivation": "현재 웹소설 플랫폼에서 사용되는 추천 알고리즘/ 검색 시스템에 대해 원하는 내용의 소설을 찾기 힘들다는 불만이 있어 이를 RAG를 사용하면 개선할 수 있을 것 같아 시작했습니다.<br><a href=\"https://jchiang1225.medium.com/book-recommendation-with-retrieval-augmented-generation-part-i-d1b415aff558\">RAG를 활용한 도서 추천 시스템</a>을 참고하여 이를 실제 서비스로 적용해보고자 하였습니다.<br>블로그에 관련 내용을 진행하면서 배운 점과 내용을 같이 정리했습니다.",
      "features": [
        "RAG (Retrieval-Augmented Generation) 기반 추천",
        "PGVector를 활용한 벡터 유사도 검색",
        "FastAPI 백엔드 + Streamlit 프론트엔드",
        "PostgreSQL 데이터베이스 연동"
      ],
      "tech": "FastAPI, Python, Streamlit, PostgreSQL, PGVector, RAG, Podman",
      "results": [
        "현재 데이터 수집 중"
      ],
      "github": "https://github.com/world970511/korea_webnovel_recommender",
      "blog": ""
    },
    "en": {
      "title": "AI Web Novel Recommendation System",
      "motivation": "I started this project to address the frustration of finding the desired content on current web novel platforms. I decided to apply RAG (Retrieval-Augmented Generation) to improve the recommendation and search systems. I also included my learnings and insights from the blog post.",
      "features": [
        "RAG-based recommendation",
        "Vector similarity search using PGVector",
        "FastAPI backend + Streamlit frontend",
        "PostgreSQL database integration"
      ],
      "tech": "FastAPI, Python, Streamlit, PostgreSQL, PGVector, RAG, Podman",
      "results": [
        "Currently collecting data"
      ],
      "github": "https://github.com/world970511/korea_webnovel_recommender",
      "blog": ""
    }
  },
  "project6": {
    "year": 2025,
    "company": "",
    "ko": {
      "title": "개인 포트폴리오 사이트 겸 블로그",
      "motivation": "이전에 사용하던 블로그를 이전할 겸 포트폴리오와 통합하였습니다.",
      "features": [
        "순수 JS,HTML,CSS 기반의 개인 포트폴리오 사이트",
        "GitHub Action을 활용한 CI/CD",
        "이력서 및 포트폴리오 다운로드 가능",
        "블로그 포스트 작성 및 관리"
      ],
      "tech": "HTML, CSS, JavaScript, Github Actions",
      "results": [
        "개인 포트폴리오 사이트와 블로그를 통합하여 사용자 경험을 개선",
        "GitHub Action을 활용한 CI/CD를 구현하여 빠른 배포를 가능"
      ],
      "github": "https://world970511.github.io/",
      "blog": ""
    },
    "en": {
      "title": "Personal Portfolio Website and Blog",
      "motivation": "Migrated from the previous blog to integrate with the portfolio.",
      "features": [
        "Personal portfolio website built with pure JS, HTML, and CSS",
        "CI/CD with GitHub Actions",
        "Resume and portfolio download",
        "Blog post creation and management"
      ],
      "tech": "HTML, CSS, JavaScript, Github Actions",
      "results": [
        "Integrated personal portfolio website and blog to improve user experience",
        "Implemented CI/CD with GitHub Actions for faster deployment"
      ],
      "github": "https://world970511.github.io/",
      "blog": ""
    }
  },
  "project5": {
    "year": 2022,
    "company": "",
    "ko": {
      "title": "KAKAO RECO",
      "motivation": "추천 시스템 알고리즘을 실제 서비스에 적용해보고 싶어서 시작한 2인 팀 프로젝트입니다.<br>행렬 분해(Matrix Factorization) 기법을 통해 희소한 사용자-작품 행렬 문제를 해결하고자 했습니다.<br>프론트엔드, 백엔드 구현 및 Heroku 배포를 통해 실제 서비스를 운영하였습니다.",
      "features": [
        "Matrix Factorization 기반 개인화 추천",
        "Jquery 무한 스크롤 구현",
        "WordCloud2.js 키워드 시각화",
        "Django 백엔드 구현"
      ],
      "tech": "Django, jQuery, Python, Matrix Factorization, Heroku , React",
      "results": [
        "희소 행렬 문제 해결",
        "Heroku 배포 및 실제 서비스 운영",
        "사용자 선호도 기반 추천 정확도 향상"
      ],
      "github": "https://github.com/world970511/kakao_webtoon_reco",
      "blog": ""
    },
    "en": {
      "title": "KAKAO RECO",
      "motivation": "This is a two-person team project initiated to apply recommendation system algorithms to a real-world service.<br>We aimed to address the sparse user-item matrix problem using Matrix Factorization techniques.<br>The project involved full-stack development and deployment on Heroku to manage a live service.",
      "features": [
        "Personalized recommendation using Matrix Factorization",
        "Infinite scroll with jQuery",
        "Keyword visualization with WordCloud2.js",
        "Django backend implementation"
      ],
      "tech": "Django, jQuery, Python, Matrix Factorization, Heroku , React",
      "results": [
        "Solved sparse matrix problem",
        "Deployed on Heroku for production",
        "Improved recommendation accuracy based on user preferences"
      ],
      "github": "https://github.com/world970511/kakao_webtoon_reco",
      "blog": ""
    }
  },
  "project4": {
    "year": 2022,
    "company": "",
    "ko": {
      "title": "Ssum-Up",
      "motivation": "엘리스 ai 서비스트랙 3기에서 진행한 두번째 팀 프로젝트입니다.<br>유튜브를 더 효율적으로 이용할 수 있도록 영어로 된 영상의 자막에서 핵심만 간추린 요약문을 제공하는 서비스로, 한글 자막이 없는 유튜브 영어 영상의 링크를 입력하면 해당 영상의 자막을 자연어 처리를 활용, 핵심만 요약해 간략하게 제공\n요약된 자막의 한국어 번역을 제공합니다.<br>팀 내에서 프론트엔드를 맡아 로그인 권한 제어 기능, My Summary 페이지 구현,main 페이지 save 버튼 기능(카테고리 선택), 로그인/회원가입 기능 등을 구현하였습니다.",
      "features": [
        "로그인 권한에 따른 서비스 접근 제한",
        "추후 다시 보고싶은 영상과 자막을 mylist에 저장, 복수의 재생목록을 만들 수 있도록 구현",
        "최근 검색해본 영상 목록(history)을 저장",
        "영어 영상의 자막을 한국어로 요약 및 영어 자막 원본 제공"
      ],
      "tech": "React MUI (Material-UI) , React-Slick, React-Player , JavaScript , HTML5 , CSS, Python ,  Django , Django REST framework,Flask ,MariaDB ,Docker/Docker-Compose , Gunicorn, MS Azure VM, Gitlab CI, Anaconda , Jupyter Notebook , Google Colab , TensorFlow , Pytorch , Numpy , Pandas , huggingface/transformers, GitLab",
      "results": [
        "MS Azure VM을 활용한 서버 구축",
        "실제 서비스 배포"
      ],
      "github": "https://github.com/Ssum-Up-project/Ssum-Up",
      "blog": ""
    },
    "en": {
      "title": "Ssum-Up",
      "motivation": "This is the second team project conducted in the 3rd Elice AI Service Track.<br>It is a service that provides a summary of the core content from English video subtitles to help users use YouTube more efficiently. When a link to an English YouTube video without Korean subtitles is entered, the service uses natural language processing to summarize the subtitles and provide a brief overview.<br>It also provides Korean translations of the summarized subtitles.<br>As a front-end developer on the team, I implemented login authorization control, the My Summary page, the save button functionality on the main page (category selection), and login/sign-up features.",
      "features": [
        "Login authorization control",
        "Mylist implementation",
        "History implementation",
        "English subtitle summary and English subtitle original text"
      ],
      "tech": "React MUI (Material-UI) , React-Slick, React-Player , JavaScript , HTML5 , CSS, Python , Django , Django REST framework,Flask ,MariaDB ,Docker/Docker-Compose , Gunicorn, MS Azure VM, Gitlab CI, Anaconda , Jupyter Notebook , Google Colab , TensorFlow , Pytorch , Numpy , Pandas , huggingface/transformers, GitLab",
      "results": [
        "Server construction using MS Azure VM",
        "Actual service deployment"
      ],
      "github": "https://github.com/Ssum-Up-project/Ssum-Up",
      "blog": ""
    }
  },
  "project3": {
    "year": 2021,
    "company": "",
    "ko": {
      "title": "Which OTT",
      "motivation": "엘리스 ai 서비스트랙 3기에서 진행한 팀 프로젝트입니다.<br>각 OTT서비스가 보유한 콘텐츠의 장르/국가/관람가 등의 데이터와 플랫폼의 오리지널/독점 콘텐츠 보유 비중을 시각화하면 각 서비스의 콘텐츠 특성을 파악할 수 있고, 이를 바탕으로 콘텐츠 기반 추천(CBF) 모델 적용 시 사용자가 원하는 콘텐츠를 많이 보유한 서비스를 추천받을 수 있을 것이라는 생각에서 진행하였습니다.<br>팀 내에서 콘텐츠 리뷰 사이트인 키노라이츠에서 2만건 이상의 데이터 크롤링 및 전처리 분석과 기획서 및 문서 작성을 맡아 진행하였습니다.",
      "features": [
        "사용자의 콘텐츠 성향 검사 후 사용자에게 알맞은 OTT 서비스 추천",
        "각 OTT서비스 분석을 통한 콘텐츠 통계 자료를 제공",
        "본인의 OTT 사용 빈도와 시간을 통해 OTT서비스 이용도 파악을 돕는 기능"
      ],
      "tech": "HTML5, CSS3, JavaScript, React, Recharts, Python, Flask, MariaDB, Pandas, Numpy, Plotly, Colab, Git, GitLab",
      "results": [
        "Plotly를 사용한 시각화 대시보드 구현 경험",
        "데이터 크롤링 및 전처리 경험",
        "데이터 EDA 및 분석 경험"
      ],
      "github": "https://github.com/world970511/elice_OTT",
      "blog": ""
    },
    "en": {
      "title": "Which OTT",
      "motivation": "This is a team project conducted during the 3rd Elice AI Service Track. The project was initiated to identify the characteristics of each OTT service by visualizing data such as genre, country, and rating, along with the proportion of original and exclusive content. Based on this, we aimed to recommend services that best match user preferences using a Content-Based Filtering (CBF) model. Within the team, I was responsible for crawling and preprocessing over 20,000 reviews from the review site Kinolights, as well as project planning and documentation.",
      "features": [
        "Recommended OTT service based on user preferences",
        "Provided content statistics for each OTT service",
        "Helped users understand their OTT usage patterns"
      ],
      "tech": "HTML5, CSS3, JavaScript, React, Recharts, Python, Flask, MariaDB, Pandas, Numpy, Plotly, Colab, Git, GitLab",
      "results": [
        "Implemented visualization dashboard using Plotly",
        "Data crawling and preprocessing experience",
        "Data EDA and analysis experience"
      ],
      "github": "https://github.com/world970511/elice_OTT",
      "blog": ""
    }
  },
  "project2": {
    "year": 2021,
    "company": "",
    "ko": {
      "title": "빅리더 아카데미 팀 MIB - AI 분석 기반 사용자 맞춤형 국립공원 탐방 서비스",
      "motivation": "빅리더 아카데미에서 국립공원공단의 도움을 받아 진행한 팀 프로젝트입니다.<br>등산 앱인 트랭글의 코스 데이터를 AI를 활용해 사용자의 체력,상황에 가장 최적화된 등산 코스를 추천하는 모델을 구축하고, 이를 확장하여 타겟층이 등산에 더 흥미를 느낄 수 있도록 SNS서비스를 제공할 것을 제안하였습니다.<br>저는 팀 내에서 데이터 전처리 및 모델 구축 역할을 맡아 행렬 분해(Matrix Factorization) 기반의 잠재 요인 협업 필터링을 적용하고, SGD(확률적 경사 하강법)를 통해 모델을 최적화하여 사용자와 코스 간의 숨겨진 특징(Latent Factors)을 학습시켰습니다. 또한, 사용자가 입력한 &#39;난이도&#39;와 &#39;시간&#39; 키워드를 필터링 조건으로 결합하여 개인화된 결과를 도출했습니다.<br>팀 프로젝트 경연대회에서 2등인 최우수상을 수상했고, 국립공원공단에서 공로상을 받았습니다.  </p>\n<p>관련자료:  <a href=\"https://www.busan.com/view/bstoday/view.php?code=2021090114512973123\">부산일보</a>",
      "features": [
        "QGIS를 활용한 데이터 처리",
        "Python을 활용한 데이터 분석",
        "Scikit-learn, Turicreate 기반 딥러닝 모델",
        "HTML,JS,Bootstrap을 활용한 웹 서비스 구축"
      ],
      "tech": "QGIS, Python, Scikit-learn, Turicreate, HTML, JS, Bootstrap",
      "results": [
        "최우수상, 국립공원공단에서 공로상을 수상",
        "QGIS를 활용한 데이터 처리 경험",
        "Scikit-learn, Turicreate를 활용한 딥러닝 모델 구축 경험"
      ],
      "github": "https://github.com/MIB0831national",
      "blog": ""
    },
    "en": {
      "title": "Big Leader Academy Team MIB - AI Analysis Based User-Centric National Park Tour Service",
      "motivation": "This project was conducted as a team initiative at the Big Leader Academy (Data Science Bootcamp) with support from the Korea National Park Service. We developed an AI-driven model that recommends optimal hiking routes by analyzing course data from the &#39;Tranggle&#39; app, tailored to a user&#39;s fitness level and specific situation. Beyond the recommendation engine, I proposed an expansion into SNS services to foster interest in hiking among our target audience. My primary role involved data preprocessing and model architecture, where I implemented Latent Factor Collaborative Filtering based on Matrix Factorization. I optimized the model using Stochastic Gradient Descent (SGD) to learn latent features between users and trails. Additionally, I integrated user-specified &#39;difficulty&#39; and &#39;duration&#39; keywords as filtering conditions to deliver highly personalized results. This project was awarded 2nd Place (Grand Prize) at the final competition and received an Achievement Award from the Korea National Park Service.  </p>\n<p>Featured In: <a href=\"https://www.busan.com/view/bstoday/view.php?code=2021090114512973123\">Bstoday</a>",
      "features": [
        "Data processing using QGIS",
        "Data analysis using Python",
        "Deep learning model with Scikit-learn, Turicreate",
        "Web service development using HTML,JS,Bootstrap"
      ],
      "tech": "QGIS, Python, Scikit-learn, Turicreate, HTML, JS, Bootstrap",
      "results": [
        "Won the second place in the team project competition and received the best award",
        "Experience in data processing using QGIS",
        "Experience in building deep learning models using Scikit-learn, Turicreate"
      ],
      "github": "https://github.com/MIB0831national",
      "blog": ""
    }
  },
  "project11": {
    "year": 2023,
    "company": "Posicube",
    "ko": {
      "title": "robi V 학습용 데이터 파이프라인 구축 및 관리",
      "motivation": "2023년부터 2025년 8월까지 OCR 솔루션인 robi V의 데이터 기획부터 수집·정제·검수·전달까지 전 과정을 담당하며, 모델 성능과 직접 연결되는 데이터 품질 관리를 주요 책임으로 수행했습니다.<br>또한, 오류 케이스 분석을 통한 신규 데이터 추가 및 데이터 품질 개선을 통해 데이터 품질을 지속적으로 개선하였습니다.<br>2023년 초부터 퇴사 전까지 담당하며 파이프라인 및 데이터를 지속적으로 개선하였습니다.\n상단 블로그 링크에서 자세한 내용 확인이 가능합니다.",
      "features": [
        "python과 머신러닝, 비동기 처리를 사용한 대용량 데이터 처리 자동화",
        "모델 오탐지 분석을 통한 취약 케이스 데이터 보강 및 품질 개선"
      ],
      "tech": "Python, Data management, Data pipeline",
      "results": [
        "분류 파이프라인 자동화: 수작업에서 자동화. ML 모델을 사용한 신분증 자동 분류 및 검수를 통해 반복 수작업 비율 90% 이상 감소",
        "라벨링 데이터 검수 및 수정 자동화: 수작업에서 자동화. JSON 정합성 검사, 좌표 오류 탐지 스크립트 등을 고도화하여 데이터 오류율을 기존 약 5%에서 0.5% 미만으로 감소",
        "ML 기반 자동 마스킹 툴 개발: 수작업에서 자동화. 외부 라벨링 인력 및 영업용 데이터 제공을 위해 개인정보를 포함한 이미지의 특정 영역을 자동으로 마스킹하도록 구현",
        "OCR 엔진을 사용한 GT 자동 생성 스크립트 작성: 수작업에서 자동화. 라벨링하기 어려운 베트남어 등을 OCR 엔진을 사용하여 자동으로 GT를 생성하도록 구현",
        "모델 오탐지 분석을 통한 취약 케이스 데이터 보강 및 품질 개선: 신분증 인식 정확도 개선"
      ],
      "github": "",
      "blog": "https://world970511.github.io/blog/posts/2025-12-30-ocr-c2.html"
    },
    "en": {
      "title": "robi V Training Data Pipeline Construction",
      "motivation": "From 2023 to August 2025, I managed the entire data lifecycle for the OCR solution &#39;robi V&#39;, covering data planning, collection, refinement, inspection, and delivery. My primary responsibility was data quality management, which is directly linked to model performance.<br>I also continuously improved data quality by incorporating new data through error case analysis and refining existing datasets.<br>You can check the details at the blog link above.",
      "features": [
        "Python and machine learning, asynchronous processing to automate large-scale data processing",
        "Data reinforcement and quality improvement through model error analysis"
      ],
      "tech": "Python, Data management, Data pipeline",
      "results": [
        "Classification Pipeline Automation: Transitioned from manual to automated. Reduced repetitive manual work by over 90% through automated ID classification and inspection using ML models.",
        "Labeling Data Inspection and Correction Automation: Transitioned from manual to automated. Reduced data error rates from approximately 5% to less than 0.5% by enhancing JSON consistency checks and coordinate error detection scripts.",
        "ML-based Auto-Masking Tool Development: Transitioned from manual to automated. Implemented automatic masking of specific areas in images containing personal information for external labeling work and sales data provision.",
        "GT Auto-Generation Script using OCR Engine: Transitioned from manual to automated. Implemented automatic Ground Truth (GT) generation using OCR engines for languages difficult to label manually, such as Vietnamese.",
        "Data Reinforcement and Quality Improvement through Model Error Analysis: Improved ID recognition accuracy."
      ],
      "github": "",
      "blog": "https://world970511.github.io/blog/posts/2025-12-30-ocr-c2.html"
    }
  },
  "project10": {
    "year": 2022,
    "company": "Posicube",
    "ko": {
      "title": "Fake Detection 학습용 데이터 파이프라인 구축",
      "motivation": "사본 판별 솔루션인 Fake Detection에 필요한 데이터 기획부터 수집·정제·검수·전달까지 전 과정을 담당하며, 모델 성능과 직접 연결되는 데이터 품질 관리를 주요 책임으로 수행했습니다.<br>또한, 오류 케이스 분석을 통한 신규 데이터 추가 및 데이터 품질 개선을 통해 데이터 품질을 지속적으로 개선하였습니다.<br>2022년부터 퇴사 전까지 담당하며 파이프라인 및 데이터를 지속적으로 개선하였습니다.\n상단 블로그 링크에서 자세한 내용 확인이 가능합니다.",
      "features": [
        "Fake Detection 학습용 데이터셋 구축 및 관리",
        "python을 사용한 대용량 데이터 처리 자동화",
        "모델 오류 케이스 및 취약점 분석을 통한 데이터 품질 관리",
        "Tool 및 라이브러리,스크립트 구현",
        "라벨링 가이드 작성 및 작업물 검수"
      ],
      "tech": "Python, Data management, Data pipeline",
      "results": [
        "데이터 중복 제거 최적화: 수작업에서 자동화. 처리 속도를 약 90% 단축 및 스토리지 효율 약 15% 개선",
        "합성 데이터 자동 생성: Pre-training에 활용 가능한 학습 데이터를 선제적으로 확보",
        "사내 라이브러리 및 마크다운 문서 검색을 지원하는 VS Code Extension 개발 : 팀 개발 생산성 향상 및 효율화",
        "라벨링 가이드 작성 및 작업물 검수/수정. 라벨링 인력 관리: Fake Detection 성능 및 인식률 개선"
      ],
      "github": "",
      "blog": "https://world970511.github.io/blog/posts/2025-12-30-fake-detection-c2.html"
    },
    "en": {
      "title": "Fake Detection Training Data Pipeline Construction",
      "motivation": "I was responsible for the entire data lifecycle—from planning and collection to refinement, inspection, and delivery—for Fake Detection, a copy discrimination solution. My primary responsibility was managing data quality, which directly impacts model performance. I focused on automating large-scale data processing using Python and continuously improved data quality by incorporating new data based on error case analysis.<br>You can check the details at the blog link above.",
      "features": [
        "Fake Detection training data pipeline construction",
        "Python-based large-scale data processing automation",
        "Data quality management through model error case and vulnerability analysis",
        "Tool and library, script implementation",
        "Labeling guide creation and workpiece inspection"
      ],
      "tech": "Python, Data management, Data pipeline",
      "results": [
        "Data deduplication optimization: Automated manual processes, reducing processing time by ~90% and improving storage efficiency by ~15%.",
        "Synthetic data generation: Proactively secured training data available for pre-training.",
        "Labeling data inspection and correction automation: Automated manual processes, reducing repetitive manual work by over 90%.",
        "Weak case data reinforcement and quality improvement through model error analysis."
      ],
      "github": "",
      "blog": "https://world970511.github.io/blog/posts/2025-12-30-fake-detection-c2.html"
    }
  },
  "project1": {
    "year": 2021,
    "company": "",
    "ko": {
      "title": "리디북스 로맨스 리뷰 감성분석",
      "motivation": "학교 인공지능 수업 기말 과제로, 리뷰를 사용한 추천 시스템에 관심이 있어 시작하게 되었습니다.<br>혼자서 진행하였고, 실제 리뷰들을 크롤링으로 수집 후 이를 활용하여 긍부정 분류 모델을 구축하였습니다.",
      "features": [
        "579,867건 리뷰 데이터 크롤링",
        "Selenium을 활용한 자동화 수집",
        "TensorFlow/Keras 기반 딥러닝 모델",
        "텍스트 전처리 및 정제"
      ],
      "tech": "TensorFlow, Keras, Selenium, BeautifulSoup, NLP, Python",
      "results": [
        "긍부정 분류 정확도 90% 달성",
        "대규모 데이터 처리 경험",
        "전체 ML 파이프라인 구축 경험"
      ],
      "github": "https://github.com/world970511/RIDIBOOKS_romance_webnovel_review_Sentiment_Analysis",
      "blog": ""
    },
    "en": {
      "title": "Ridibooks Romance Review Sentiment Analysis",
      "motivation": "I started this project as a final assignment for a university AI course, motivated by my interest in recommendation systems that utilize reviews. I conducted the project independently, collecting actual review data through web crawling and using it to build a sentiment analysis model for positive/negative classification.",
      "features": [
        "Crawled 579,867 review data",
        "Automated collection using Selenium",
        "Deep learning model with TensorFlow/Keras",
        "Text preprocessing and cleaning"
      ],
      "tech": "TensorFlow, Keras, Selenium, BeautifulSoup, NLP, Python",
      "results": [
        "Achieved 90% accuracy in sentiment classification",
        "Large-scale data processing experience",
        "Built complete ML pipeline"
      ],
      "github": "https://github.com/world970511/RIDIBOOKS_romance_webnovel_review_Sentiment_Analysis",
      "blog": ""
    }
  }
}